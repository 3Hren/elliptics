Introduction. Key/value storage. Distributed hash table.
----------------------------------------------------------------------------------------------------------

Distributed key/value storages allow to lookup data using external keys generated either
by user or using a data hashing. The latter has own name: distributed hash tables.
Such systems implement similar to hash table data access pattern (key/value) and any node
can efficiently determine data location without need to contact external servers.

Keys are consistently mapped to participating storage nodes so that this information
could be easily retrieved by clients, which then are able to determine a peer node to
fetch data from or upload to.

Such mappings are called routing table, which contains node address and key range it knows about.
It is not required that each node knew about all others, instead nodes can cover larger ranges,
than physically allowed by its storage, but its private routing table can contain links to other
nodes hidden behind visible one. In this case lookup protocol requires quite complex discovery
process, bound by the storage network topology.

The most popular network structure implements a ring (usually 2^128 or larger) of keys, where
each node becomes just a boundary between key ranges covered by given node. For example we can
spread one-byte-long keys among 3 nodes by assigning them 0, 0x55 and 0xaa IDs and forcing
nodes to locally store data which belongs to [0, 0x55), [0x55, 0xaa), [0xaa, 0) ranges accordingly.

If each node is connected to all others data lookup will be implemented at O(1) time (or better say
O(1) of number of network messages). It is also possible that nodes are connected only to limited
set of remote peers, depending on connection heuristics lookup time may be limited by other values.

In any case network topology must guarantee that there will be a path to some node which is
'more closer' to requested key than others. There are different algorithms (which effectively form
network topology) which limit number of hops to be contacted before needed node is found, lookup
time (or number of network messages needs to be sent) can be limited, for example, by following numbers:

 O(log N) - this network topology is implemented in Chord[1].
 	    In this design each node is connected to each 2th, 4th, 8th and so on node.
 O(d-th root of N) - when using d-dimensional cortesian coordinates as node IDs.
 		     This model is implemented in CAN (Content addressable network) [2]

There are many other metrics used to detemine node 'closeness' to requirested key.



Elliptics network - a distributed key/value storage. Distributed hash table storage.
----------------------------------------------------------------------------------------------------------

Elliptics network uses quite simple ID ring structure described above.
Each node is assigned a unique ID which splits ID range of its neighbour such way that
node stores all keys which are bigger than node's ID.

With described ranges above we will get following network topology:
node ID 0x00: stored key range [0, 0x55)
node ID 0x55: stored key range [0x55, 0xaa)
node ID 0x00: stored key range [0xaa, 0)

Each client receives similar routing table usually at startup, but can regularily refresh it.

Routing table is not required to contain always correct information. By default each node
in elliptics network stores address of every other peer, which allows to implement O(1) lookup
time while requiring O(N) memory to store all remote node's addresses. In common elliptics
network deployment it is limited by tens-to-hundreds nodes, so this is acceptible, although
it is quite simple to change it (back to old days) O(log N) number of nodes stored and
O(log N) lookup time.


Elliptics network as DHT.
----------------------------------------------------------------------------------------------------------

Elliptics network is a distributed key/value storage. By default key is generated by the library
as a data hash, thus elliptics network is also a distributed hash table.

System uses fixed-sized keys (by default 128 bits) generated by OpenSSL exported hashes.
There are two strategies used to generate key and store data:
 * transactions and history logs
 * data rewrite

The former implements kind of append-only storage, when old data is never overwritten.
Instead we store tranactions (which are indexed by hashing its content) in separate objects,
and maintain an update history log for the main object we write data to. At any given time
there is no data object which contains all writes in it, instead client reads its history
log and finds out transactions which updated requested data range, client then download
them in parallel and store locally to requested destination (file or buffer).

This looks like this:
write range [0, 100] to '/etc/shadow' => transaction 1 (its key is hash of the data being written)
write range [50, 150] to '/etc/shadow' => transaction 2
write range [75, 100] to '/etc/shadow' => transaction 3
and so on.

In this scenario system will contain a single history log for '/etc/shadow' (stored in the object
with key being equal to hash('/etc/shadow')) and number of separate transactions propbably spread
over multiple nodes.

When client wants to read some data he will fetch this history log and find out which transactions
cover the range he is interested in. For example for data range [25, 60] we will have to read
transactions 1 and 2, and second one will overwrite part of the first one, or we can read just
part of the first transaction, namely [0, 50) and [50, 60] from the second transaction.

This technique is similar to column databases (like Apache Cassandra), where we store multiple
values for the same key, and then select proper value (or column), and only then select data.

All transactions embed a timestamp, which is used for merging process when. For details see below.


Another data storage technique is quite common rewrite-in-place method, when data is stored
in the object which corresponds to the latest update state. Each write goes directly into the data
without ability to roll it back. In this case there is a single data object per key and some meta
information associated with it (like latest update time and so on).

This model is effectively the same as what we see in filesystem - each write into the file goes
to disk rewriting old file's content.
This mode is quite useful when we want external applications to work with data stored in elliptics network,
and which do not work with elliptics library itself, i.e. which are not capable of understanding how
to deal with transactions.

For example musical or video streaming, which may ask elliptics network about object location
and then directly connect to the HTTP server on the node we found.

Depending on how data is acually stored on disk, this external application can directly access data
of perform some additional steps. And here we come to modular architecture of the elliptics network IO server.




Elliptics architecture.
------------------------------------------------------------------------------------------------------

Elliptics network is a client-server application. While they share core codebase, their main difference
is in operation direction.

Client sends command to server, which either executes them or forwards to other nodes according to the
specified key. Client-server protocol is described in header files and documentation, so that multiple
clients could implement it. Also elliptics network library provides two-levels of API to work with data,
which effectively implement above protocol.

To date there are two frontends, i.e. systems which can send commands to elliptics server:
 * http frontend
 * command-line tools to read/write/stat/remove/execute remote command/kill humans/whatver else


HTTP frontend is a feature-rich system which handles data upload/download/removal and statistics requests.
It supports both transaction based storage as well as storing data in classical rewriteable objects.
Also it can embed some additional metadata with data (like putting timestamps together with data for
better client-side cache management), can download data directly or receive a link to the http server
which (when properly configured) will handle direct content distribution.

HTTP frontend is implemented as a FastCGI application, and can be easily deployed with any HTTP server
supporting the technology. Archive and packages come with lighttpd config file, which describes all
supported features.

HTTP frontend, besides global configuration, can receive special URI commands which can change or influence
data processing. Thus it is possible for external application or client to perform steps they need
to handle the data. URI parameters can be changed in frontend configuration and are also decribed in
attached configuration file.


Command line tools include following applications:
 * dnet_ioclient - main client application which is capable of reading/writing data, removing objects,
 			requesting per-client statistics, send commands to remote nodes for execution and
			lookup hosts
 * dnet_hparser - transaction history parser application, which is useful for off-site log checking
 * dnet_notify - simple application which shows elliptics network update notification processing
 			It registers to receive notification about all updates happened to specified keys
 * dnet_stat - statistics gathering tool. It will ask remote servers about filesystem/memory/LA stats
 			and will display it on timed base


Further development efforts will be concentrated on another elliptics network frontend: POHMELFS
It will provide POSIX filesystem interface to the distributed storage, so that applications which
worked with local filesystems could benefit from distributed facilities elliptics network provide
like data redundancy and fault tolerance as well as forget about storage size limitation.


Elliptics network is effectively a library which implements lots and lots of features of distributed storage
like message routing and key lookup, network IO processing, transaction allocation, processing and cleanup
and so on. All operations are wrapped into nice API calls available in libelliptics.so

There are two API levels: high-level interface, which works with files on local disk and low-level API
which works with opaque data. The former implements blocking reading/writing of data from/to local file
as well as its removing, while low-level interface is completely asynchronous. Client registers a callback
which may be executed multiple times until transaction is completed and optionally acked by remote server node.

There is a C++ wrapper for the most widely used client operations: blocking read/write/remove file, blocking
read/write data to/from provided pointer and async non-blocking data read/write which requires appropriate
callback registration.

There is also a Python wrapper (on top of C++ code) which imlpements the same operation, but later I dropped
async IO support because of complexities it requires from python programmer to handle GIL and sync with other
Python threads. This can be recovered back though, but I'm not sure async IO is a common programming model
in Python.



Another side of the elliptics network is its storage nodes. It is those entities which actually store data on disk
or other persisten (or not) media.
Elliptics network IO server supports following IO backends:
 * file IO backend - in this low-level storage system each data transaction, each object is a separate file on disk.
 	Thus any external application can easily access whatever we wrote into the system - it just opens file by
	the path which consists of server root directory, directory prefix (calculated from the key we use to store
	given data object) and key itself (converted into string of hex digits). Directory prefix is virtually part
	of the key (preconfigured number of bits we copy from the beggining of the key) converted into string of hex
	digits.
 * Database IO backend. We used to support BerkeleyDB, but it was dropped due to extremely slow performance compared
 	to TokyoCabinet database [3]. BDB was upto 100 times (!) slower than TC, although it provides ACID capabilities
	which are missed in TokyoCabinet. Also TC is not very stable on huge amount of data and starts swapping its
	index to disk when amount of data becomes too big.
 * EBLOB IO backend. libeblob [4] is a simple enough low-level storage which puts data into huge blob files on disk.
 	It was designed to allow O(1) lookup of file/offset of the needed object and not to swap index from memory.
	Data being written can be aligned and optionally checksummed, external application can then verify them and
	defragment, i.e. remove those entries which were marked as deleted.
	Append-only storage allows to implement very fast writes and with (optionally) always-in-memory index - fast reads.
	To date this is the fastest low-level IO backend supported in elliptics network [5].

Because of simplicity provided by modular elliptics network architecture it is quite simple to add own IO backends
which will store data whatever we would like to. For example there was an IO backend which stored files in 'tar' files.
It takes just several days to implement a backend using existing sources.

It is possible that database IO backend will be dropped from elliptics network, since eblob backend provides noticebly
faster access although with less features. Although libeblob provides listing and even limited possibility to externally
modify blobs (without locks, so parallel simultaneous writes can damage themself), this functionality is far away
from database lookup/get/put, but in case of TokyoCabinet we have to lock the whole database to update its records,
which is not acceptible, so effectively its advantages do not play a significant role in elliptics storage.



Elliptics network implementaion details: data redundancy, fault tolerance, transctions, versions and snapshots and so on.
---------------------------------------------------------------------------------------------------------------------------

Elliptics network is a distributed key/value storage. Having more keys associated with the same data being written means
that multiple copies of the written block will exist in the storage. Using data hashes to generate keys and having
multiple hash functions registered for given block we end up having multiple copies of the data.

Knowing ID distribution among nodes in the storage it is possible to tweak hash generation function the way it will
produce hash, which will belong to the interested node. This is a rather complex task to be implemented on client, and
instead we introduced virtual datacenters.

Virtual datacenter is a set of nodes combined into some logical group, where nodes may or may not actually be physically
groupped together. System modifies hash generated by common function by changing its first 4 bytes to be equal to the
datacenter number.

Thus VDC feature allows to specify preconfigured prefix in every transaction ID - the first 4 bytes. Nodes which have
their first ID bytes equal to VDC number will receive all transactions indexed by appropriate hash function.

Here is a virtual datacenter example.

Let's suppose we have two transformation functions setup on client: dc1_sha1 and dc2_sha1.
They will produce sha1 hash of the data with the first bytes set to 1 and 2 accordingly.

Now let's suppose we add two nodes with IDs being equal to 0x01000000 and 0x01000080 and then two nodes with
0x02000000 and 0x02800000 IDs. Now there will be two transactions made with above transformation functions with its
first 4 bytes set either to 1 or 2, so there will be a guaranteed copy in each virtual datacenter.

Now we can implement multiple VDCs with different ID's first bytes for every datacenter we want to work with.
If we only need to have 2 copies, but there are more than 2 datacenters, nodes can be spread between those VDCs.
'2' is arbitrary here of course.

This feature also allows to implement geographical linking of the requests. Let's suppose some application receives data
read request from Moscow, it can check whether its set of transformation functions contains the one with the ID assigned to Moscow.
If there is such a function, it can be used first to obtain object ID and to fetch it from Moscow-local servers instead of
going to New York, where main datacenter lives. If Moscow storage does not contain our requested object, we will use second
transformation function(s), which will 'point' to main storage cluster.


Fault tolerance in elliptics network.
Having multiple copies of the data is not enough be considered as a fault tolerance system. Storage also has to be able
to recover from disk/node/datacenter loss. Hash table operates with keys and related data values, so there must be
an external way to determine how given key was generated and how many copies should exist in the storage - since client
controls number of copies each written object will have, there is no central place which could tell us how many copies
exist for any particular transaction.
To maintain such information elliptics network has per-object metadata. It contains a variable-length set of data required
for object recovery, and actually may contain other objects client would like to have associated with given data object.
Elliptics library exports methods to read/write/search particular metadata types within metadata object. In particular
transaction history is just another metadata type as well as name used to generate transaction ID and set of hash functions.

Having this information system can determine all keys used for update transactions, and thus it is able to check whether
transactions are present in the storage or not. If some of them were missed, it can be cloned from existing copies thus
recovering number of copies.

Another problem is so called network split (or split-brain) problem. When object with the same key is updated in parallel
on different nodes and eventually those nodes contact each other. Which data object we should fetch? What if transactions
on different nodes overwrite the same region?

Elliptics network uses transaction history log to solve this problem. Each transaction has a timestamp and by default
merge process will create new history log which will be sorted by timestamp. So any subsequent read will return more
recent data while it is still possible to get older transaction if there are some problems.
Checking utility is able to call external callbacks to merge histories. For example it can use content check to determine
which object to use, or it can contact external service to determine what data is correct.

This functionality is lost when no transactions are used, i.e. when we rewrite data object in-place. It is still possible
though to select that one, which will be used after merge, by timestamp sorting or by using external library call.


With enabled transactions we effectively gets snapshotting support - at any given time client is able to request
transaction history log and fetch those transaction which she wants thus looking into the past and creating snapshots
of the object.

Transactions are also used for versioning. Essentially versioning is the same snapshotting mechanism, which is indexed by
version number. Elliptics network modifies key to put version number at the end of the key (we use 4 bytes for version number),
thus each transaction for given object will have version embeedded in its key.
When client request given object with some version, he can parse history log and select those transactions which
have required version (higher or lower if interested).

Versioning is supported out-of-the-box in HTTP frontend, while more generic snapshotting is a task for developers which
use elliptics library - there are exported calls to parse transaction history logs so that anyone could select transactions
they want. By default system selects the latest transactions.



'write-always-succeed' and 'eventual constistency' model.
------------------------------------------------------------------------------------------------------------------
When performing write, elliptics network will put data according to the specified (or calculated by hash) key.
This key will be mapped to node which currently holds a control over ID range which includes given key. If network
fails or recover id-range-to-node-address mapping (also known as routing table) may change, but still if there is
at least one accessible node there will be an address to put data to.

Elliptics client will send data to available node which currently covers ID range which include a key being written,
thus implementing 'write-always-succeed' model. HTTP frontend can check number of accessible node and if it is less
than preconfigured number write will fail, although this does not change the model: data will be written to some nodes,
and when network topology changes or when new nodes are added this keys may fall out of the ID range maintained by the given node.

Thus it is possible that every single key may become unaccessible because of routing table changes.

There is a set of tools to deal with this problem, namely 'dnet_check_merge' - this util will ask given node to return
a list of keys it contains which do not belong to the currently maintained ID range, i.e. those keys which should be
copied to another nodes to become accessible again. And surprisingly dnet_check_merge will try to move them to the
proper locations.

It is called 'merge' because it will also try to merge tranaction logs if object in the storage has a history log
which differs from log on the given node. This may happen when network temporarily breaks and clients write some
updates to the available nodes. When network recovers, those updates continue to live on the old nodes, which
squeeze their ID range to old values thus making some keys unaccesible. 'dnet_check_merge' will try to move them
out to the right nodes.

No checking tool is called automatically on network failure or recover, instead this task has to be scheduled by external
decision like administrivia or timed check. Thus system will be made consistent eventually - when appropriate tasks
are started and completed its work (which may take a while).

Elliptics network uses eventual consistency model not only because of its convinience from development and administration
sides, but also to reduce synchronization bottlenecks when storage is extended. Consider a situation when single
node contains several terabytes of data and it is being updated. When we add new node it has to copy large amount
of data from this loaded node. This by itself takes a lot of time (especially if network is loaded or shaped) and
noticebly increases load to the node in question. We can not guarantee when data will be available on new node
and we can not permit updates of the keys to be copied to new node, since new node is not yet ready to handle
requests. Thus trying to force 'immediate consistency' model we increase load exactly when we want to decrease it,
also we do not allow new nodes to take at least some load from the storage. And we do not gain a time: copying
terabytes will take hours if not days anyway.

In 'eventual consistency' model new node will start handling requests immediately. Since it is most likely empty,
all read requests will naturally fail, that's why distributed storages are highly motivated to have multiple data
copies. Thus client will try different key and will eventually get data from another node.
Write requests will go directly to new node and later will be merged with tranaction log stored on the old node.

It is possible that with complex enough data structure written to the storage with eventual consistency model
such simple timestamp based transaction merge may not be enough. That's why there is a possibility to use external
library to merge data.


Distributed hash table pros and cons.
------------------------------------------------------------------------------------------------------------------------------
Using key/value approach and managing key-to-node mapping allows to make extremely scalable storage systems.
But is this that simple in real life?

Main pros of the distributed hash table is client's ability to map key to storage node's address within limited
number of network lookups (or even without them at all if routing table contains all nodes in the storage and it
is not outdated). Thus DHT storage does not have a single point of failure.

Also using mathematically-proven hash function allows to evenly distribute data across all storage node balancing
IO operations. Data tends to wear with time and older content is usually less interested and thus less accessible
than newer one. In DHT old and new data will be spread over all nodes in the storage and likely there will be no
over or underloaded servers.

But that's the theory. Practice, as usual, is quite different.
In practice network regularily fails, thus forcing updates to come to other nodes, which in turn makes them
overloaded during update and later when data will have to be moved to the right nodes.

DHT is effectively a hash table thus it has the same problems: it does not actually scale. To change hash table
size one has to rehash all content of the table. With some tricks we can rehash only part of the storage content,
but still we will have to copy large amount of data each time storage topology changes.

This is needed to maintain mentioned above even distribution, but in some practical cases we do not need that level
of distribution fairness.

Another problem with DHT is its unflexibility in key management. Let's consider a simple case when we want some
data to be copied 3 times and some objects to have 2 or 4 copies. Or when we want to increase number of copies
of some popular data to remove some load from the nodes which currently host it.
Since there is no central entity which holds information about every object's metadata, namely how many copies
there are in the storage and how many keys refer to given data, it is likely not possible for external clients
to determine other keys requested data is accessible through.

This problem is solved by introducing external caching - a layer on top of main storage which will hold a smaller
number of keys (and associated data objects) and which will fall back to main storage when data was not found in cache.
It is possible to implement multiple cache levels, but each new level increases lookup time: cache miss has a high
price always.


Thus DHT usage is limited by the cases which involves rather rare (or controllable) storage extensions,
since each new node requires data copy. In every DHT installation there will likely be an external caching layer,
which in turn has to be configured and administrated. This may also limit deployment scenarios.


Elliptics network production usage includes cases where data has either a good cache hit (to simplify external
cache deployment) or where data does not require cache at all (for example when we have clients which in turn
are connected to own caches). To date we store hundreds of millions of objects distributed over multiple physical
datacenters which regulary face network and disk problems. Usually we use 3 copies in at least 3 different datacenters.
The largest deployed elliptics network storage is planned to grow up to 1 billion of records (not including copies)
within a year.

Further plans include elliptics network metadata storage extensions for maximum performance. DHT systems and more
generic key/value storage should distributed per-object metadata or put it close to objects themself, which heavily
decreases batch operation performance, namely storage checkers and recovery process which will greatly benefit
if switched from per-object processing to blocks of keys. Tuning this data for maximum performance will allow
to descrease recovery and check times.

Further development efforts will be concentrated on another elliptics network frontend: POHMELFS
It will provide POSIX filesystem interface to the distributed storage, so that applications which
worked with local filesystems could benefit from distributed facilities elliptics network provide
like data redundancy and fault tolerance as well as forget about storage size limitation.

Another meaningful project is to implement a distributed lock management system, which will allow to create
truly distributed transactions. To date when multi-keyed transaction (like writing data with several copies)
partially fails (like when only one copy was successfully written), it is up to client to decide how to recover.
Also multiple clients updating the same object may interfer and different copies of the same object will have
different transaction history. And while it is still possible to fix, this requires manual processing.

Distributed locks (or more generally distributed increasing counter) allows to fix similar problems.
There are multiple implementations (like zookeeper as the most widely used), which may or may not fit the needs
and should be checked to fit the requirements.


Conclusion.
-------------------------------------------------------------------------------------------------------------------
Elliptics network storage allows users to access a set of physically distributed servers through flat addressing model in
decentralized network environment. Key/value distributed storage provides an efficient method of accessing data with limited set of
constraints. This was proved by practical deployment of DHT storage which hosts hundreds of millions of records and provides
high access rate and flexibility.
Elliptics network implements different frontends suitable for solving various real-life storage problems.
Modular architecture and pluggable IO backend allow to implement storage system with required functionality and finely
tuned storage performance.
Highlighted pros and cons of distributed hash table technology in general and elliptics network implementation in particular
make a wide area for maneuver during storage system selection in regards of required functionality and flexibility.
